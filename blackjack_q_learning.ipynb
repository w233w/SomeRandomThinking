{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"664 project.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["## BlackJack Implementation"],"metadata":{"id":"x1GCRqLBQpbe"}},{"cell_type":"code","source":["import random\n","import gym\n","from collections import defaultdict"],"metadata":{"id":"iZ2lgt4unozE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Global BlackJack environment.\n","env = gym.make('Blackjack-v0')\n","\n","# Global function to calculate the average of a list\n","avg = lambda lst : sum(lst) / len(lst)"],"metadata":{"id":"eI2neu_Hd4kO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def generate_an_episode(q_table, weights, rewards):\n","\n","    # Used to store an episode\n","    episode = []\n","\n","    state = env.reset()\n","\n","    done = False\n","    while not done:\n","\n","        # Take in an action\n","        action = random.choices(population=[0, 1], weights=weights[state], k=1)[0]\n","\n","        # Interact with the environment to generate rewards and next states\n","        next_state, reward, done, _ = env.step(action)\n","\n","        # Store state, action, and reward to episode\n","        episode.append([state, action, reward])\n","\n","        # Update state\n","        state = next_state\n","\n","    return episode"],"metadata":{"id":"f9BHgXKmnu1x"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def reversely_traversal_episode(q_table, weights, rewards, episode):\n","\n","  # Init the reward\n","  R = 0.0\n","\n","  # Episode reverse traversal\n","  episode.reverse()\n","  for t in range(len(episode)):\n","    # Get state, action, and reward from each round in an episode   \n","    state, action, reward = episode[t]\n","\n","    # Reward function\n","    # R = R + γ * reward\n","    # γ = 1 in this case because every game is independent. So ignored here.\n","    R += reward\n","\n","    # Add R into the collection of rewards.\n","    rewards[state][action].append(R)\n","\n","    # Updating Q-table\n","    q_table[state][action] = avg(rewards[state][action])\n","\n","    # The Winning decision has higher chance be used on the next episode\n","    good_choise = max(q_table[state], key = q_table[state].get)\n","    weights[state][good_choise] = 0.9\n","    weights[state][1 - good_choise] = 0.1"],"metadata":{"id":"b70xxRL7jVJM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def monte_carlo(sample_size = 50000):\n","\n","  # Used on Q_learning\n","  q_table = defaultdict(lambda: {0:0.0, 1:0.0})\n","\n","  # When the AI knows nothing, the weight of action is evenly distributed.\n","  weights = defaultdict(lambda: [0.5, 0.5])\n","\n","  # Used to collect rewards\n","  rewards = defaultdict(lambda: {0:[], 1:[]})\n","\n","  for i in range(sample_size):\n","\n","    # Generate episode\n","    episode = generate_an_episode(q_table, weights, rewards)\n","\n","    reversely_traversal_episode(q_table, weights, rewards, episode)\n","\n","  return q_table"],"metadata":{"id":"bgHj08czdXN1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["raw_policy = monte_carlo()\n","\n","# gym.Env.BlackJackEnv only accept two input actions: 0 & 1 (False & True）， O represent stand and 1 represent hit.\n","# Here we convert from {state: [0's reward, 1's reward]} to {state: 0 or 1} \n","act = lambda dic : max(dic, key = dic.get)\n","policy = {key: act(val) for key, val in raw_policy.items()}"],"metadata":{"id":"4wSCPT9RdgGP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Test Policy"],"metadata":{"id":"Iyh9k1N_SoxX"}},{"cell_type":"code","source":["# A single round of game\n","def sample_simulation(policy):\n","  init_state = env.reset()\n","  current_state = init_state\n","\n","  done = False\n","  while not done:\n","    # Using policy get from AI to play the game, and collect the result.\n","    action = policy[current_state]\n","    current_state, reward, done, _ = env.step(action)\n","\n","  # Only collect initial state\n","  return init_state, reward"],"metadata":{"id":"xBkDdJULHrU9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def policy_check(policy, sample_size = 100000):\n","\n","  # Result\n","  result_set = {}\n","  for iter in range(sample_size):\n","\n","    # Get init state of a game and it's result.\n","    state, reward = sample_simulation(policy)\n","\n","    # Store result\n","    if state in result_set:\n","      result_set[state].append(reward)\n","    else:\n","      result_set[state] = [reward]\n","\n","  return result_set"],"metadata":{"id":"Iw0VMSlBvYyj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Used to summarize win/tie/loss rate from policy_check() by state.\n","def over_all_rate(game_result):  \n","  rate = {}\n","  for result_state, result in game_result.items():\n","    win_rate = result.count(1.0) / len(result)\n","    tie_rate = result.count(0.0) / len(result)\n","    loss_rate = result.count(-1.0) / len(result)\n","    rate[result_state] = [win_rate, tie_rate, loss_rate]\n","  return rate"],"metadata":{"id":"pxUU2vOYJb-j"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Win rate & loss rate\n","\n","Around 44.5% win rate, 47.5% loss rate.\n","\n","Note that deviation may occur."],"metadata":{"id":"1nTt7nGIyoPw"}},{"cell_type":"code","source":["# Print win and loss rate.\n","def win_loss_rate(game_result):\n","  rate = over_all_rate(game_result)\n","\n","  #result[0] is where win_rate in each state.\n","  win_rate = avg([result[0] for result in rate.values()])\n","  print(\"Win  Rate: \" + str(win_rate))\n","\n","  loss_rate = avg([result[2] for result in rate.values()])\n","  print(\"Loss Rate: \" + str(loss_rate))"],"metadata":{"id":"I_vGpmAqln71"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Check policy and get policy win/loss rate.\n","overall_ratio = policy_check(policy=policy, sample_size=100000)\n","win_loss_rate(overall_ratio)"],"metadata":{"id":"Xb8c7Sc7QiRR","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1651703953107,"user_tz":240,"elapsed":14543,"user":{"displayName":"Yang Ye","userId":"16470661890232050766"}},"outputId":"cd8605e4-04e0-46ec-902c-2df26e8156f8"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Win  Rate: 0.44763889242363675\n","Loss Rate: 0.4742434536263865\n"]}]}]}